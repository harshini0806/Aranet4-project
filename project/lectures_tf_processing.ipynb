{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "loaded_data = np.load('../datasets/windows.npz', allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the column names\n",
    "column_names = loaded_data['column_names']\n",
    "\n",
    "# Convert the loaded data back to a dictionary of lists of DataFrames, using the column names\n",
    "windows_df = {label: [pd.DataFrame(array, columns=column_names) for array in arrays_list] \n",
    "              for label, arrays_list in loaded_data.items() if label != 'column_names'}\n",
    "\n",
    "# Loop through windows_df and set 'Datetime' as the index\n",
    "for label, windows_list in windows_df.items():\n",
    "    for i, window in enumerate(windows_list):\n",
    "        # Convert 'Datetime' to a datetime object\n",
    "        window['Datetime'] = pd.to_datetime(window['Datetime'])\n",
    "\n",
    "        # Set 'Datetime' as the index\n",
    "        windows_df[label][i] = window.set_index('Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of each DataFrame in the windows dictionary\n",
    "for label, windows_list in windows_df.items():\n",
    "    print(f\"Label: {label}\")\n",
    "    for i, window in enumerate(windows_list):\n",
    "        print(f\"Window {i}: {window.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_df['Song'][0].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each DataFrame in windows_df\n",
    "for label, windows_list in windows_df.items():\n",
    "    for window in windows_list:\n",
    "        \n",
    "        # Ensure that the 'co2' column is numeric\n",
    "        window['co2'] = pd.to_numeric(window['co2'], errors='coerce')\n",
    "        \n",
    "        # Create rolling average for CO2 with a window of 3 (change this as needed)\n",
    "        window['co2_ma3'] = window['co2'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "        # Create rolling average for CO2 with a window of 3 (change this as needed)\n",
    "        window['co2_ma5'] = window['co2'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "        # Create rolling average for CO2 with a window of 3 (change this as needed)\n",
    "        # window['co2_std5'] = window['co2'].rolling(window=5, min_periods=1).std()\n",
    "        \n",
    "        # Create shift (lag) features for CO2\n",
    "        # window['co2_std5'] = window['co2_std5'].fillna(method='bfill')\n",
    "\n",
    "        # Create shift (lag) features for CO2\n",
    "        window['co2_lag1'] = window['co2'].shift(1).fillna(method='bfill')\n",
    "        \n",
    "        # Create difference feature for CO2\n",
    "        # window['co2_diff'] = window['co2'].diff().fillna(0)\n",
    "\n",
    "# Example: Display the first DataFrame for 'Song' label after preprocessing\n",
    "windows_df['Song'][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the loaded data back to a dictionary of lists of NumPy arrays, excluding 'column_names'\n",
    "windows = {label: list(arrays) for label, arrays in windows_df.items() if label != 'column_names'}\n",
    "\n",
    "\n",
    "for label, windows_list in windows.items():\n",
    "    print(f\"Label: {label}, Number of windows: {len(windows_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows['Song'][0].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to collect all the 2D arrays\n",
    "all_arrays = []\n",
    "\n",
    "# Loop through each label in windows_df\n",
    "for label, windows_list in windows_df.items():\n",
    "    for window in windows_list:\n",
    "        # Convert the DataFrame to a NumPy array and add it to the list\n",
    "        all_arrays.append(window.values)  # Using .values to convert DataFrame to NumPy array\n",
    "\n",
    "# Stack all 2D arrays into a 3D array (assuming they all have the same number of columns)\n",
    "# If the number of rows varies, this step will fail and you might need to handle variable shapes differently\n",
    "try:\n",
    "    X = np.stack(all_arrays, axis=0)\n",
    "    print(\"Successfully stacked all DataFrames into a 3D NumPy array.\")\n",
    "except ValueError as e:\n",
    "    print(\"Error stacking arrays. This might be due to differing shapes:\", str(e))\n",
    "    # If shapes differ, use a general Python list or an array of objects\n",
    "    X = np.array(all_arrays, dtype=object)\n",
    "    print(\"Stored arrays in an object dtype array.\")\n",
    "\n",
    "print(\"Shape of the final array X:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the 3D arrays and labels\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for label, windows_list in windows.items():\n",
    "    for window in windows_list:\n",
    "        # Convert each DataFrame to a NumPy array and append to the list\n",
    "        X_list.append(window)\n",
    "        # Append the corresponding label to the label list\n",
    "        y_list.append(label)\n",
    "\n",
    "# Convert the list of 3D arrays to a single 3D array (tensor)\n",
    "X = np.array(X_list)\n",
    "\n",
    "# Convert the label list to a NumPy array\n",
    "y = np.array(y_list)\n",
    "\n",
    "# Print the shapes of the resulting arrays\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `y` is our classifer, in this case it classifies lectures in `X` as `0` for `Song` or `1` for `Chen`. So if we take $n$ slices from each lecture intervals in each lecture $X_i \\in X$ to subsets $S_j \\in S$ such that $U_{j=0}^{n} S_j = X_i$, we will have to ensure labels from `y` are assigned accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = [i for i in range(1, n + 1) if n % i == 0]\n",
    "    return divisors\n",
    "\n",
    "# Find divisors of 76\n",
    "divisors_of_76 = find_divisors(76)\n",
    "print(\"Divisors of 76:\", divisors_of_76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming X is your original array with shape (25, 76, 10)\n",
    "T = X.copy()\n",
    "\n",
    "# Verify that the second dimension is divisible by 2\n",
    "if T.shape[1] % 4 != 0:\n",
    "    raise ValueError(\"The number of records in each layer must be divisible by 2\")\n",
    "\n",
    "# Reshape T directly to the new shape\n",
    "# We reshape to (total number of new layers, new number of records per layer, number of features)\n",
    "# Total number of new layers = original layers * 2 because we split each layer into 2\n",
    "# New number of records per layer = original number of records per layer / 2\n",
    "new_layers = T.shape[0] * 4\n",
    "new_records_per_layer = T.shape[1] // 4\n",
    "T = T.reshape(new_layers, new_records_per_layer, T.shape[2])\n",
    "\n",
    "# Duplicate each element in y to match the new layer count\n",
    "y = np.repeat(y, 4)\n",
    "\n",
    "print(\"New shape of T:\", T.shape)\n",
    "print(\"New shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first and the last columns from the 3D array X\n",
    "# T = T[:, :, 1:-2]\n",
    "import numpy as np\n",
    "\n",
    "# Assuming T is your 3D NumPy array and has a sufficient number of columns\n",
    "# Calculate the indices of the columns to keep\n",
    "cols_to_keep = [i for i in range(T.shape[2]) if i not in (2, 3, 7, 8)]\n",
    "\n",
    "# Slice the array to keep only the desired columns\n",
    "T = T[:, :, cols_to_keep]\n",
    "\n",
    "print(\"New shape of T after removing columns 2, 3, 6, and 7:\", T.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "T = T.astype(np.float64)\n",
    "# Encode labels: 'Song' as 1, 'Chen' as 0\n",
    "y = np.array([1 if label == 'Song' else 0 for label in y])\n",
    "# Scale the features using StandardScaler\n",
    "\n",
    "# Reshape X to 2D array\n",
    "T_2d = T.reshape(-1, T.shape[-1])\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "T_scaled = scaler.fit_transform(T_2d)\n",
    "\n",
    "# Reshape X_scaled back to 3D array\n",
    "T_scaled_3d = T_scaled.reshape(T.shape[0], T.shape[1], T.shape[2])\n",
    "\n",
    "# Split into training and test/validation sets\n",
    "T_train, T_test, y_train, y_test = train_test_split(T_scaled_3d, y, test_size=20, random_state=42, stratify=y)\n",
    "\n",
    "print(\"X_train shape:\", T_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", T_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[19,8]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(T_train, y_train, epochs=15, validation_data=(T_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5),\n",
    "    xlim=[0, 30], ylim=[0, 2], xlabel='Epochs', grid=True,\n",
    "    style=['r', 'r--', 'b', 'b-*'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(T_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict(T_test)\n",
    "y_proba.round(2)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_classification(y_true, y_pred, average='macro'):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1}\n",
    "\n",
    "# Example usage (ensure y_test and y_pred are defined appropriately)\n",
    "metrics = evaluate_classification(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing RNN Sequenceing for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the split index for 90% of the time steps\n",
    "split_index = int(0.8 * 19)  \n",
    "\n",
    "# Splitting the training data\n",
    "X_train = T_train[:, :split_index, :]\n",
    "y_train = T_train[:, split_index:, :]\n",
    "\n",
    "# Splitting the testing data\n",
    "X_test = T_test[:, :split_index, :]\n",
    "y_test = T_test[:, split_index:, :]\n",
    "\n",
    "# Print shapes to confirm the setup\n",
    "print(\"X_train shape:\", X_train.shape)  \n",
    "print(\"y_train shape:\", y_train.shape)  \n",
    "print(\"X_test shape:\", X_test.shape)    \n",
    "print(\"y_test shape:\", y_test.shape)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# # Define the model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.SimpleRNN(32, activation='relu', input_shape=(15, 7)),  # Input layer\n",
    "#     tf.keras.layers.Dense(16, activation='relu'),  # Hidden layer\n",
    "#     tf.keras.layers.Dense(4 * 7, activation=None)  # Output layer to predict 16 timesteps, each with 20 features\n",
    "# ])\n",
    "# model.add(tf.keras.layers.Reshape((4, 7)))  # Reshape output to match (16, 20)\n",
    "\n",
    "# Define the LSTM model with dropout\n",
    "model = tf.keras.Sequential([\n",
    "    # Adding dropout and recurrent dropout to the LSTM layer\n",
    "    tf.keras.layers.LSTM(32, activation='relu', input_shape=(15, 8),\n",
    "                         dropout=0.2, recurrent_dropout=0.2),\n",
    "    # Adding L2 regularization to the Dense layer\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    # Output layer to predict 8 timesteps, each with 20 features\n",
    "    tf.keras.layers.Dense(4 * 8, activation=None)\n",
    "])\n",
    "model.add(tf.keras.layers.Reshape((4, 8)))  # Reshape output to match (8, 20)\n",
    "\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Define optimizer\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=2000, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Flatten the predictions and actual values for metric calculations\n",
    "y_true_flat = y_test.reshape(-1)\n",
    "y_pred_flat = y_pred.reshape(-1)\n",
    "\n",
    "# Calculate metrics\n",
    "test_mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "test_r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Test MAE: {test_mae:.4f}')\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n",
    "print(f'Test R-squared: {test_r2:.4f}')\n",
    "\n",
    "\n",
    "# Extracting predictions for CO2 (assuming it's the first column in the output)\n",
    "y_pred_co2 = y_pred[:, :, 0]  # Adjust the index if CO2 is not the first column\n",
    "y_true_co2 = y_test[:, :, 0]\n",
    "\n",
    "# Flatten the CO2 predictions and actual values\n",
    "y_true_co2_flat = y_true_co2.reshape(-1)\n",
    "y_pred_co2_flat = y_pred_co2.reshape(-1)\n",
    "\n",
    "# Calculate metrics for CO2\n",
    "test_mae_co2 = mean_absolute_error(y_true_co2_flat, y_pred_co2_flat)\n",
    "test_rmse_co2 = np.sqrt(mean_squared_error(y_true_co2_flat, y_pred_co2_flat))\n",
    "test_r2_co2 = r2_score(y_true_co2_flat, y_pred_co2_flat)\n",
    "\n",
    "# Print the metrics for CO2\n",
    "print(f'Test MAE for CO2: {test_mae_co2:.4f}')\n",
    "print(f'Test RMSE for CO2: {test_rmse_co2:.4f}')\n",
    "print(f'Test R-squared for CO2: {test_r2_co2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def calculate_r2_per_feature(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the R-squared score for each feature across all timesteps.\n",
    "    Args:\n",
    "    y_true (numpy.ndarray): True values of the test set.\n",
    "    y_pred (numpy.ndarray): Predicted values from the model.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with feature indices as keys and R-squared scores as values.\n",
    "    \"\"\"\n",
    "    r2_scores = {}\n",
    "    for feature_index in range(y_true.shape[2]):  # Assuming the last dimension represents features\n",
    "        y_true_feature = y_true[:, :, feature_index].reshape(-1)\n",
    "        y_pred_feature = y_pred[:, :, feature_index].reshape(-1)\n",
    "        r2_scores[feature_index] = r2_score(y_true_feature, y_pred_feature)\n",
    "    return r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_test and y_pred are already defined and contain the test and predicted data respectively.\n",
    "r2_scores = calculate_r2_per_feature(y_test, y_pred)\n",
    "print(\"R-squared scores for each feature:\", r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature names if available or use generic names\n",
    "feature_names = ['Feature {}'.format(i) for i in range(y_test.shape[2])]  # Adjust or replace with actual names\n",
    "\n",
    "# Extract R2 values and sort by value\n",
    "r2_values = [r2_scores[i] for i in sorted(r2_scores)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_names, r2_values, color='blue')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('R-squared Score')\n",
    "plt.title('R-squared Score for Each Feature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to make room for label rotation\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5),\n",
    "    xlim=[0, 500], ylim=[0, 2], xlabel='Epochs', grid=True,\n",
    "    style=['r', 'r--', 'b', 'b-*'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of test samples\n",
    "n_test_samples = y_test.shape[0]\n",
    "\n",
    "# Setup the plot\n",
    "plt.figure(figsize=(14, 40))\n",
    "for i in range(n_test_samples):\n",
    "    plt.subplot(n_test_samples, 1, i + 1)\n",
    "    plt.plot(y_true_co2[i], label='Actual CO2', marker='o')\n",
    "    plt.plot(y_pred_co2[i], label='Predicted CO2', marker='x')\n",
    "    plt.title(f'Test Sample {i+1}')\n",
    "    plt.xlabel('Time Steps (Minutes)')\n",
    "    plt.ylabel('CO2 Levels')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
